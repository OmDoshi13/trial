# Trenkwalder AI HR Chatbot

An AI-powered HR assistant chatbot built with **RAG (Retrieval-Augmented Generation)** that answers questions from company documents (PDF, TXT, Markdown) and fetches real-time employee data via tool/function calling â€” all running **100% locally** using Ollama.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Web UI (FastAPI + HTML/JS/CSS)             â”‚
â”‚         http://localhost:8000                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Agent Orchestrator                       â”‚
â”‚   â€¢ Intent detection (HR query vs document query)    â”‚
â”‚   â€¢ LLM-based tool calling (TOOL_CALL: ...)         â”‚
â”‚   â€¢ Conversation history management                  â”‚
â”‚   â€¢ Pre-emptive document search for context          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                              â”‚
       â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAG Pipeline     â”‚        â”‚   HR Tool Functions   â”‚
â”‚  â€¢ PDF/TXT/MD     â”‚        â”‚   (Mock HR API)       â”‚
â”‚    ingestion      â”‚        â”‚   Port 8001            â”‚
â”‚  â€¢ Text chunking  â”‚        â”‚                        â”‚
â”‚  â€¢ Embedding via   â”‚        â”‚  â€¢ get_vacation_days  â”‚
â”‚    nomic-embed-textâ”‚        â”‚  â€¢ get_sick_leave     â”‚
â”‚  â€¢ ChromaDB vector â”‚        â”‚  â€¢ get_upcoming_leave â”‚
â”‚    store           â”‚        â”‚  â€¢ get_employee_profileâ”‚
â”‚  â€¢ Semantic search â”‚        â”‚  â€¢ get_payslip_info   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                              â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Ollama (Local)   â”‚
           â”‚  â€¢ llama3.2 (LLM) â”‚
           â”‚  â€¢ nomic-embed-textâ”‚
           â”‚    (Embeddings)    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

### Core Capabilities
- **Document Q&A (RAG)**: Upload and query PDF, TXT, and Markdown documents
- **Dynamic HR Data**: Real-time employee information via mock HR API (vacation days, sick leave, payslip, profile)
- **Smart Intent Detection**: Automatically routes questions to document search or HR tools based on keywords
- **Tool Calling**: LLM decides when to call external tools using `TOOL_CALL:` pattern
- **Conversation Memory**: Maintains chat history within a session for contextual follow-ups

### Document Support
- **Company documents auto-ingested** on startup from `documents/` folder
- **Runtime document upload** via the web UI (PDF, TXT, MD)
- **Semantic search** â€” finds relevant content even when exact keywords don't match
- **Multi-document support** â€” query across all uploaded documents simultaneously

### Web UI
- Modern dark-themed chat interface
- **Upload Doc** button for runtime document ingestion
- **Documents** sidebar showing all ingested documents
- **New Chat** button to reset conversation and start fresh
- Real-time typing indicator while processing
- Markdown rendering in bot responses

### HR Tools (Mock API)
| Tool | Description | Example Query |
|------|-------------|---------------|
| `get_vacation_days` | PTO/vacation balance | "How many vacation days do I have?" |
| `get_sick_leave` | Sick leave balance | "What's my sick leave balance?" |
| `get_upcoming_leave` | Scheduled future leave | "Do I have any upcoming leave?" |
| `get_employee_profile` | Employee info (name, dept, manager) | "What's my employee profile?" |
| `get_payslip_info` | Salary and payslip details | "What is the salary of Om Doshi?" |

### Employee Mapping
| Name | Employee ID |
|------|-------------|
| Om Doshi | EMP001 (default) |
| Klahm Sebestian | EMP002 |

## ğŸ“‹ Prerequisites

- **Python 3.11+**
- **Ollama** installed and running locally ([install here](https://ollama.com))

## ğŸš€ Quick Start

### 1. Install Ollama & pull models

```bash
# Install Ollama (macOS)
brew install ollama

# Start Ollama
ollama serve

# Pull required models
ollama pull llama3.2
ollama pull nomic-embed-text
```

### 2. Setup the project

```bash
# Clone and navigate to project
cd trenkwalder-chatbot

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Copy env file
cp .env.example .env
```

### 3. Run the chatbot

**Web UI (recommended):**
```bash
python3 -m src.main web
```
- Chat UI: http://localhost:8000
- Mock HR API: http://localhost:8001 (started automatically)

**CLI mode:**
```bash
python3 -m src.main cli
```

**Ingest documents only:**
```bash
python3 -m src.main ingest
```

## ğŸ“ Project Structure

```
trenkwalder-chatbot/
â”œâ”€â”€ documents/                  # Company documents (auto-ingested on startup)
â”‚   â”œâ”€â”€ company_overview.md
â”‚   â”œâ”€â”€ employee_benefits.md
â”‚   â”œâ”€â”€ faq.md
â”‚   â”œâ”€â”€ it_security_policy.md
â”‚   â””â”€â”€ onboarding_guide.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # Core agent â€” intent detection, tool calling, RAG
â”‚   â”‚   â””â”€â”€ prompts.py          # System prompt & answer templates
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ chunker.py          # Text chunking (500 chars, 50 overlap)
â”‚   â”‚   â”œâ”€â”€ embedder.py         # Ollama embedding generation
â”‚   â”‚   â”œâ”€â”€ ingest.py           # Document ingestion pipeline
â”‚   â”‚   â””â”€â”€ loaders.py          # PDF, TXT, MD file loaders
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ vector_store.py     # ChromaDB vector store + semantic search
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ hr_tools.py         # HR tool function definitions
â”‚   â”‚   â””â”€â”€ mock_hr_service.py  # FastAPI mock HR API (port 8001)
â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”œâ”€â”€ cli.py              # Terminal chat interface
â”‚   â”‚   â””â”€â”€ web.py              # FastAPI web app + API endpoints
â”‚   â”œâ”€â”€ config.py               # Pydantic settings (env-based config)
â”‚   â””â”€â”€ main.py                 # Entry point (web/cli/ingest commands)
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html              # Web UI (HTML + CSS + JS, single file)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ chroma_data/                # ChromaDB persistent storage (auto-created)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ”§ How It Works

### 1. Document Ingestion
```
PDF/TXT/MD files â†’ Loader â†’ Text Chunks (500 chars) â†’ Embeddings (nomic-embed-text) â†’ ChromaDB
```
- Documents in `documents/` are auto-ingested when the server starts
- Users can upload additional documents at runtime via the web UI

### 2. Query Processing
```
User Question
     â”‚
     â–¼
Intent Detection (keyword-based)
     â”‚
     â”œâ”€â”€ HR Question? â”€â”€â†’ Skip doc search, let LLM call HR tools
     â”‚
     â”œâ”€â”€ Document Question? â”€â”€â†’ Pre-emptive semantic search in ChromaDB
     â”‚                          Attach top 5 chunks as context
     â”‚
     â””â”€â”€ General/Greeting? â”€â”€â†’ LLM responds directly
     â”‚
     â–¼
LLM (llama3.2) generates response
     â”‚
     â”œâ”€â”€ TOOL_CALL detected? â”€â”€â†’ Execute tool â†’ Feed result back to LLM
     â”‚
     â””â”€â”€ Direct answer â”€â”€â†’ Return to user
```

### 3. Tool Calling Flow
```
User: "What is the salary of Om Doshi?"
  â†’ Orchestrator detects HR keywords ("salary", "Om Doshi")
  â†’ Skips document search
  â†’ LLM receives system prompt with employee mapping (Om Doshi â†’ EMP001)
  â†’ LLM outputs: TOOL_CALL: get_payslip_info(employee_id="EMP001")
  â†’ Orchestrator calls Mock HR API: GET http://localhost:8001/api/payslip/EMP001
  â†’ Result fed back to LLM
  â†’ LLM generates human-friendly answer with salary details
```

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| LLM | Ollama + llama3.2 | Text generation, tool calling |
| Embeddings | Ollama + nomic-embed-text | Semantic text embeddings |
| Vector Store | ChromaDB | Document storage & similarity search |
| Web Framework | FastAPI | REST API + web UI serving |
| Frontend | HTML + CSS + Vanilla JS | Single-file chat interface |
| CLI | Typer + Rich | Terminal interface |
| Config | Pydantic Settings | Type-safe environment configuration |
| HTTP Client | httpx | Async API calls to Ollama & HR service |
| PDF Parsing | PyMuPDF (fitz) | PDF text extraction |

## ğŸ¯ Design Decisions

1. **Fully local with Ollama** â€” No cloud API keys needed, all data stays on-device
2. **ChromaDB** â€” Lightweight persistent vector store, ideal for local development
3. **LLM-driven tool calling** â€” The LLM decides when to call tools via `TOOL_CALL:` pattern in system prompt; no hard-coded if/else routing
4. **Keyword-based intent pre-filtering** â€” HR keywords skip document search to avoid irrelevant context polluting tool-call decisions
5. **Strategy pattern for loaders** â€” Each document format (PDF, TXT, MD) has its own loader; easy to add new formats
6. **Single-file frontend** â€” All HTML, CSS, and JS in one `index.html` for simplicity
7. **Mock HR service as separate FastAPI app** â€” Runs on port 8001, simulates a real microservice architecture
8. **Pydantic Settings for config** â€” Type-safe, environment-variable-based configuration with `.env` support

## ğŸ“ Assumptions

- Ollama is running locally on port 11434
- Company documents are in the `documents/` directory
- Mock HR data covers two employees: EMP001 (Om Doshi) and EMP002 (Klahm Sebestian)
- Single-user chatbot (no authentication)
- Conversation history is session-based (resets on server restart or "New Chat")

## ğŸš§ What I'd Improve With More Time

- **Streaming responses** (SSE) for real-time token-by-token output
- **Persistent chat history** with session management
- **Multi-user support** with authentication
- **Docker containerization** for one-command deployment
- **RAG evaluation** using RAGAS framework
- **Caching layer** for repeated queries
- **Observability** with OpenTelemetry tracing
- **Real HR API integration** replacing mock service
- **Cloud deployment** (AWS Lambda + API Gateway + S3 + RDS)
- **Unit & integration tests** with pytest
- **Document deletion** from the vector store
- **Hybrid search** combining semantic + keyword (BM25) retrieval